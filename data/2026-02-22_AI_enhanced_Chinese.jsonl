{"id": "2602.16929", "categories": ["quant-ph", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.16929", "abs": "https://arxiv.org/abs/2602.16929", "authors": ["Sanidhay Bhambay", "Prakash Murali", "Neil Walton", "Thirupathaiah Vasantam"], "title": "Adaptive Aborting Schemes for Quantum Error Correction Decoding", "comment": null, "summary": "Quantum error correction (QEC) is essential for realizing fault-tolerant quantum computation. Current QEC controllers execute all scheduled syndrome (parity-bit) measurement rounds before decoding, even when early syndrome data indicates that the run will result in an error. The resulting excess measurements increase the decoder's workload and system latency. To address this, we introduce an adaptive abort module that simultaneously reduces decoder overhead and suppresses logical error rates in surface codes and color codes under an existing QEC controller. The key idea is that initial syndrome information allows the controller to terminate risky shots early before additional resources are spent. An effective scheme balances the cost of further measurement against the restart cost and thus increases decoder efficiency.\n  Adaptive abort schemes dynamically adjust the number of syndrome measurement rounds per shot using real-time syndrome information. We consider three schemes: fixed-depth (FD) decoding (the standard non-adaptive approach used in current state-of-the-art QEC controllers), and two adaptive schemes, AdAbort and One-Step Lookahead (OSLA) decoding. For surface and color codes under a realistic circuit-level depolarizing noise model, AdAbort substantially outperforms both OSLA and FD, yielding higher decoder efficiency across a broad range of code distances. Numerically, as the code distance increases from 5 to 15, AdAbort yields an improvement that increases from 5% to 35% for surface codes and from 7% to 60% for color codes.\n  To our knowledge, these are the first adaptive abort schemes considered for QEC. Our results highlight the potential importance of abort rules for increasing efficiency as we scale to large, resource-intensive quantum architectures.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.16948", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.16948", "abs": "https://arxiv.org/abs/2602.16948", "authors": ["Matthias Christandl", "Omar Fawzi", "Ashutosh Goswami"], "title": "Fault-tolerant interfaces for quantum LDPC codes", "comment": "64 pages, 8 figures", "summary": "The preparation of a quantum state using a noisy quantum computer (gate noise strength $\u03b4$), will necessarily affect an O($\u03b4$)-fraction of the qubits, no matter which protocol is used. Here, we show that fault-tolerant quantum state preparation can be achieved with constant space overhead improving on previous constructions requiring polylogarithmic overhead.\n  To achieve this, we add to the toolbox of fault-tolerant schemes for circuits with quantum input and output. More specifically, we construct fault-tolerant interfaces that decrease the level of protection for quantum low-density parity-check (LDPC) codes. When information is encoded in multiple code blocks, our interfaces have constant space overhead.\n  In our decoder construction that change the level of protection by an arbitrary amount, we circumvent bottlenecks to error pileup and overhead by gradual lowering of the level of encoding at the same time as we increase the number of blocks on which decoding is carried out simultaneously.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.17035", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2602.17035", "abs": "https://arxiv.org/abs/2602.17035", "authors": ["Jing-Hui Huang", "Xiang-Yun Hu"], "title": "Weak-Value Amplification for Longitudinal Phase Measurements Approaching the Shot-Noise Limit Characterized by Allan Variance", "comment": "9 pages, 5 figures", "summary": "We report a quantitative evaluation of weak-value amplification (WVA) for longitudinal phase measurements using Allan variance analysis. Building on a recent double-slit interferometry experiment with real weak values [Phys. Rev. Lett. 134, 080802 (2025)], our Allan variance analysis demonstrates measurement of a few attosecond time delay approaching the shot noise limit at short averaging intervals of $T$ = $0.01-0.1$ s, representing two orders of magnitude variance reduction compared to the $T=300$ s operating point in prior implementations. We demonstrate that the Allan-variance noise floor scales with the inverse of the detected photon number $1/N_r$, confirming shot-noise-limited operation with WVA. Furthermore, this $1/N_r$ scaling experimentally validates that WVA can outperform conventional measurement under fixed detected photon number and detector saturation, in the presence of technical noise, as theoretically predicted [Phys. Rev. Lett. 118, 070802 (2017)]. Our results provide rigorous, quantitative evidence of the near-optimal noise performance achievable with WVA, establishing a new benchmark for precision optical metrology. This advancement is particularly relevant to applications such as gravitational-wave detection, where signals predominantly occupy the high-frequency regime ($>10$ Hz).", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
